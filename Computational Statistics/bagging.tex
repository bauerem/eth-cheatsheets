\section*{Bootstrap Aggregating (Bagging)}
\textbf{Bagging for Regression:} For data $(X_1,Y_1),...,(X_n,Y_n)$ and base procedure $\hat g(\cdot): \mathbb{R}^p\to \mathbb{R}$, take $B$ bootstrap samples $\hat g_{bag}(x) = \frac 1 B \sum_{b=1}^B \hat g^{*b}(x)$ where $\hat g^{*b}$ is the estimate based on the $b$-th bootstrap sample. No pruning, since variance of single tree not a problem because of averaging. Linear predictions are the same under bagging, so only interesting for non-linear estimates. For regression can only improve or stay the same. \\
\textbf{Bagging for Classification:} $\hat g(\cdot): \mathbb{R}^p \to \{1, ..., k\}$. $\hat g (x) = \text{argmax}_{k=1,...,K} \sum_{b=1}^B \mathds{1}_{\hat g^{*b}(x)=k}$ (majority vote). Can also get class probability: $\hat p_k^{bag}(x) = \frac{1}{B} \sum_{b=1}^B \hat p_k^{*b}(x)$. Can also be formulated as $\hat g^{bag}(x) = \text{argmax}_{k=1,...,K} \hat p_k^{bag}(x)$ (better if interested in class probabilities, sometimes even helps accuracy). Bagging a good classifier can improve performance, but bagging a bad classifier can decrease performance. \\

